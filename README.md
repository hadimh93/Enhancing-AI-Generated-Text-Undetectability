\documentclass{article}
\usepackage{hyperref}
\usepackage{enumitem}

\title{README: Enhancing the Undetectability of AI-Generated Text Using Explainability Techniques}
\date{}

\begin{document}

\maketitle

\section*{Introduction}
This repository contains the code and resources for the project titled "Enhancing the Undetectability of AI-Generated Text Using Explainability Techniques". The project explores how explainability methods, specifically SHAP values, can be used to identify and modify features that contribute to the detectability of AI-generated text, making it less distinguishable from human-written content.

\tableofcontents

\section{Dataset}
We used the dataset from the CLIN33 Shared Task on the Detection of Text Generated by Large Language Models. The dataset includes both AI-generated and human-written texts across multiple genres (news, reviews, and tweets) in English and Dutch.

\section{Project Structure}
\begin{itemize}
    \item \textbf{Code/}: Contains the main code scripts.
    \item \textbf{Results/}: Stores the outputs, including modified texts and performance metrics.
    \item \textbf{most\_effective\_tokens.csv}: CSV file with tokens and their mean absolute SHAP values.
    \item \textbf{README.md}: Project documentation.
\end{itemize}

\section{Requirements}
\begin{itemize}
    \item Python 3.x
    \item Required Python packages (see \texttt{requirements.txt}): \texttt{pandas}, \texttt{numpy}, \texttt{nltk}, \texttt{scikit-learn}, \texttt{xgboost}, \texttt{gensim}, \texttt{shap}, \texttt{matplotlib}, \texttt{seaborn}, \texttt{openai}
\end{itemize}

\section{Usage}
\subsection*{Setup}
Clone the repository:
\begin{verbatim}
git clone https://github.com/yourusername/ai-text-undetectability.git
cd ai-text-undetectability
\end{verbatim}

Install the required packages:
\begin{verbatim}
pip install -r requirements.txt
\end{verbatim}

Download NLTK resources:
The script will automatically download required NLTK resources (stopwords, punkt). Ensure you have internet connectivity.

Obtain OpenAI API Key:
Sign up for an OpenAI account and obtain an API key.

Set the OPENAI\_API\_KEY environment variable:
\begin{verbatim}
export OPENAI_API_KEY='your-api-key'
\end{verbatim}

\subsection*{Running the Code}
Navigate to the Code/ directory:
\begin{verbatim}
cd Code
\end{verbatim}

Run the main script:
\begin{verbatim}
python main.py
\end{verbatim}

This script will:
\begin{itemize}
    \item Load and preprocess the data.
    \item Train the XGBoost classifier.
    \item Perform explainability analysis using SHAP.
    \item Apply token replacement strategies.
    \item Evaluate the model's performance before and after modifications.
    \item Save the results and generate plots.
\end{itemize}

View Results:
\begin{itemize}
    \item Modified texts and replacements are saved in \texttt{Results/strategyX\_results.csv} for each strategy.
    \item Performance metrics and plots are saved in the \texttt{Results/} directory.
\end{itemize}

\section{Methodology}
\subsection*{Data Preprocessing}
\begin{itemize}
    \item Lowercasing: All text is converted to lowercase.
    \item Removal of URLs: URLs are removed using regular expressions.
    \item Removal of Special Characters: Non-alphabetic characters are removed.
    \item Tokenization: Text is tokenized using NLTK's word tokenizer.
    \item Stop Word Removal: Stop words are removed based on the language (English or Dutch).
    \item Reconstruction: Tokens are joined back into text for vectorization.
\end{itemize}

\subsection*{Model Training}
\begin{itemize}
    \item Feature Extraction: TF-IDF vectorization with a maximum of 1,000 features.
    \item Classifier: XGBoost classifier trained to distinguish between AI-generated and human-written texts.
    \item Evaluation Metrics: Precision, recall, F1-score, and accuracy.
\end{itemize}

\subsection*{Explainability Analysis}
\begin{itemize}
    \item SHAP Values: Used to identify the most influential tokens contributing to the model's predictions.
    \item Token Importance: Mean absolute SHAP values are calculated for each token.
    \item Visualization: Top tokens are visualized in bar plots.
\end{itemize}

\subsection*{Token Replacement Strategies}
\begin{itemize}
    \item \textbf{Strategy HSR (Human Similar Word Replacement)}: Replace influential tokens with similar words used by humans. Word2Vec model trained on human-written texts from the training set. No similarity threshold applied; average cosine similarity is above 0.8.
    \item \textbf{Strategy PSR (Part-of-Speech Consistent Replacement)}: Similar to HSR but ensures replacements have the same POS tag. POS tagging performed using NLTK's POS tagger.
    \item \textbf{Strategy GPTR (GPT-4 Replacement)}: Use GPT-4o mini to generate human-like replacements. Prompts constructed to guide the model in replacing tokens.
    \item \textbf{Strategy GGPTR (Genre-Specific GPT-4 Replacement)}: Extends GPTR by including genre-specific context in prompts.
\end{itemize}

\section{Results}
The classifier achieved an initial accuracy of 80\% on the test set. After applying the token replacement strategies, the accuracy decreased, indicating increased undetectability:
\begin{itemize}
    \item Strategy HSR: Accuracy reduced to 73\%.
    \item Strategy PSR: Accuracy reduced to 76\%.
    \item Strategies GPTR and GGPTR: Accuracy reduced to 78\%.
\end{itemize}
Detailed performance metrics and plots are available in the \texttt{Results/} directory.

\section{Future Work}
\begin{itemize}
    \item \textbf{Human Evaluation}: Assess the naturalness of modified texts through human judgment.
    \item \textbf{Generalizability}: Apply methodologies across different genres and languages.
    \item \textbf{Advanced Techniques}: Explore other explainability methods and models.
    \item \textbf{Real-Time Applications}: Develop tools for live text generation with enhanced undetectability.
    \item \textbf{Robustness Testing}: Evaluate strategies on various classification models.
\end{itemize}

\section{References}
\begin{itemize}
    \item Bird, S., Klein, E., \& Loper, E. (2009). \textit{Natural Language Processing with Python}. O'Reilly Media.
    \item Chen, T., \& Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. \textit{Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, 785-794.
    \item Lundberg, S. M., \& Lee, S.-I. (2017). A Unified Approach to Interpreting Model Predictions. \textit{Advances in Neural Information Processing Systems}, 4765-4774.
    \item Mikolov, T., Chen, K., Corrado, G., \& Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. \textit{arXiv preprint arXiv:1301.3781}.
    \item Loper, E., \& Bird, S. (2002). NLTK: The Natural Language Toolkit. \textit{Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics}, 63-70.
\end{itemize}

\section{Acknowledgments}
We would like to thank the organizers of the CLIN33 Shared Task for providing the dataset used in this study.

\end{document}
